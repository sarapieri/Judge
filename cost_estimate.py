import argparse
import tiktoken
from tqdm import tqdm 
import json 
from math import ceil
from PIL import Image

''' GPT-4V Cost Estimator '''

def compute_tokens_image(width, height, detail) -> int:
    """
    Calculate the token cost for processing an image based on specified detail level and dimensions.
    
    For 'low' detail, the function returns a fixed cost of 85 tokens. This is suitable for tasks requiring
    minimal detail.

    For 'high' detail, the function first ensures that the largest dimension of the image does not exceed 2048 pixels,
    scaling down if necessary while preserving the aspect ratio. It then adjusts the dimensions again to make the smallest
    side exactly 768 pixels, maintaining the aspect ratio. The area of the image is then divided into 512x512 tiles,
    with each tile costing 170 tokens. The total cost is the number of tiles multiplied by 170, plus an additional 85 tokens
    for processing overhead. Image uploads max of to 20MB per image.

    Args:
        width (int): The width of the image in pixels.
        height (int): The height of the image in pixels.
        detail (str): The desired detail level ('low' or 'high').

    Returns:
        int: The total number of tokens required to process the image.

    Raises:
        ValueError: If 'detail' is not 'low' or 'high'.
    """

    if detail == 'low':
        return 85  # Low detail images cost a fixed 85 tokens

    elif detail == 'high':
        # Scale the image to fit within a 2048x2048 square while maintaining aspect ratio
        if max(width, height) > 2048:
            scale_factor = 2048 / max(width, height)
            width = int(width * scale_factor)
            height = int(height * scale_factor)

        # Scale the image such that the shortest side is 768 pixels
        scale_factor = 768 / min(width, height)
        width = int(width * scale_factor)
        height = int(height * scale_factor)

        # Compute the number of 512x512 squares needed
        num_tiles_width = ceil(width / 512)
        num_tiles_height = ceil(height / 512)
        num_tiles = num_tiles_width * num_tiles_height

        # Each 512px square costs 170 tokens, plus an additional 85 tokens
        total_tokens = num_tiles * 170 + 85
        return total_tokens

    else:
        raise ValueError("Detail must be 'low' or 'high'")

def load_image_and_compute_tokens(image_path):
    """
    Loads an image from the specified path and calculates the token costs for processing it
    at both high and low detail levels.

    This function supports PNG, JPEG, WEBP, and non-animated GIF formats. It raises exceptions
    if the image format is unsupported or if an animated GIF is encountered, as animation handling
    is not supported. For each image, it computes the number of tokens required for processing
    in both high and low detail settings by considering its dimensions.

    Args:
        image_path (str): The path to the image file.

    Returns:
        tuple: A tuple containing two integers:
               - The first integer is the number of tokens required for high detail processing.
               - The second integer is the number of tokens required for low detail processing.

    Raises:
        ValueError: If the image format is not supported or if the image is an animated GIF.
        IOError: If the image file cannot be opened or found, indicating a potential issue with the file path or permissions.
    """
    # Supported formats
    supported_formats = {'PNG', 'JPEG', 'WEBP', 'GIF'}

    try:
        # Load the image
        with Image.open(image_path) as img:
            if img.format not in supported_formats:
                raise ValueError(f"Unsupported image format: {img.format}. Supported formats are PNG, JPEG, WEBP, and GIF.")
            
            # Ensure the GIF is not animated
            if img.format == 'GIF' and getattr(img, "is_animated", False):
                raise ValueError("Animated GIFs are not supported.")

            width, height = img.size
            
            # Compute tokens for both high and low detail levels
            tokens_high = compute_tokens_image(width, height, 'high')
            tokens_low = compute_tokens_image(width, height, 'low')

            return tokens_high, tokens_low

    except IOError:
        raise IOError("The file could not be opened or found. Please check the file path and ensure the format is correct.")

def num_tokens_from_string(encoder, string: str) -> int:
    """
    Calculates the number of tokens generated by encoding a given text string using a specified encoder.

    Args:
        encoder: An encoder object capable of encoding strings into tokens. This object must have an
                 `encode` method.
        string (str): The text string to encode.

    Returns:
        int: The number of tokens that the text string encodes into.

    Raises:
        Exception: Re-throws any exceptions that occur during encoding.
    """
    try:
        num_tokens = len(encoder.encode(string))
        return num_tokens
    except Exception as e:
        raise Exception(f"Error occurred: {e}")

def main(args):
    """
    Processes a dataset of images and text prompts, calculating the cost based on token usage
    for both high and low detail image processing. Outputs cost statistics to the console.
    
    Args:
        args: Command line arguments including model name, file paths, cost parameters, and output text assumptions.
    """
    image_high_token_count = 0
    image_low_token_count = 0
    input_token_count_text = 0
    cost_high = 0
    cost_low = 0 
    tot_output_token_count = 0
    encoder = tiktoken.encoding_for_model(args.model_name)

    # Load Prompt
    with open(args.prompt_file, 'r') as file:
        prompt = file.read().strip() 

    prompt_tokens = num_tokens_from_string(encoder, prompt)

    # Assuming output token count calculation is fixed
    output_token_count = num_tokens_from_string(encoder, args.possible_output_text)

    # Load JSON dataset
    with open(args.data_file, 'r') as file:
        data = json.load(file)

    print(f'Processing Cost Estimate of {args.data_file} with {args.model_name} (prompt: {args.prompt_file})...\n')

    for entry in tqdm(data):
        input_token_count_text = 0
        input_token_count_low = 0
        input_token_count_high = 0

        # Add prompt tokens to input token counts
        input_token_count_text = prompt_tokens

        # Add text tokens 
        if entry["prompt"]:
            image_text = "Text:\n" + entry["prompt"]
            input_token_count_text += num_tokens_from_string(encoder, image_text)

        # Calculate tokens for image processing
        tokens_high, tokens_low = load_image_and_compute_tokens(entry["image"])
        image_high_token_count += tokens_high
        image_low_token_count += tokens_low

        # Total input tokens for low and high details
        input_token_count_low += input_token_count_text + tokens_low
        input_token_count_high += input_token_count_text + tokens_high

        # Calculate costs for low and high detail settings
        cost_high += input_token_count_high * args.cost_per_token_input + output_token_count * args.cost_per_token_output
        cost_low += input_token_count_low * args.cost_per_token_input + output_token_count * args.cost_per_token_output
        tot_output_token_count += output_token_count

    average_cost_high_per_image = cost_high / len(data) if data else 0
    average_cost_low_per_image = cost_low / len(data) if data else 0
    average_tokens_high_per_image = image_high_token_count / len(data) if data else 0
    average_tokens_low_per_image = image_low_token_count / len(data) if data else 0

    # Print all statistics
    print("--------------------------------")
    print('Cost Estimate Results:')
    print(f"Total Images: {len(data)}")
    print(f"Prompt tokens: {prompt_tokens}")
    print(f"Total Tokens (High Detail Images): {image_high_token_count}")
    print(f"Total Tokens (Low Detail Images): {image_low_token_count}")
    print(f"Average Tokens (High Detail per Image): {average_tokens_high_per_image:.2f}")
    print(f"Average Tokens (Low Detail per Image): {average_tokens_low_per_image:.2f}")
    print(f"Input Token Count (Low Detail) One sample: {input_token_count_low}")
    print(f"Input Token Count (High Detail) One Sample: {input_token_count_high}")
    print(f"Total Output Tokens: {tot_output_token_count}, Per sample: {tot_output_token_count/len(data)}")
    print(f"Total Cost for High Detail: ${cost_high:.2f} (Average per Image: ${average_cost_high_per_image:.4f})")
    print(f"Total Cost for Low Detail: ${cost_low:.2f} (Average per Image: ${average_cost_low_per_image:.4f})")
    print("--------------------------------")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Process some images and texts.")
    parser.add_argument('--model_name', type=str, default="gpt-4", help='Name of the model')
    parser.add_argument('--prompt_file', type=str, default='./prompt_gpt-4_V2.txt', help='File path for the prompt text')
    parser.add_argument('--data_file', type=str, default='./dataset.json', help='File path for the dataset')
    parser.add_argument('--cost_per_token_input', type=float, default=0.00001, help='Cost per input token. Source: https://openai.com/pricing')
    parser.add_argument('--cost_per_token_output', type=float, default=0.00003, help='Cost per output token. Source: https://openai.com/pricing')
    parser.add_argument('--possible_output_text', type=str, default="{\n  \"safe_combination\": false,\n  \"problem\": [\"deception\", \"ads\"]\n}", help='Possible output text for token calculation')
    args = parser.parse_args()
    main(args)
        